{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from importlib import reload\n",
    "from gensim import models\n",
    "from sklearn import metrics\n",
    "import spacy\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Activation, Convolution2D, MaxPooling2D, Flatten, Input, Dropout, Concatenate\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEHIND_THE_WORDS_DIR = \"./\"\n",
    "DATA_DIR = os.path.join(BEHIND_THE_WORDS_DIR, \"data\")\n",
    "USING_GPU = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.load_word2vec import load_word2vec\n",
    "\n",
    "w2v_model_path = os.path.join(BEHIND_THE_WORDS_DIR, \"data/gensim/word2vec-google-news-300.gz\")\n",
    "word2vec = load_word2vec(w2v_model_path, \"http://127.0.0.1:7070\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TOKEN_LENGTH = 384\n",
    "\n",
    "def process_text(text, max_token_length=None):\n",
    "  if max_token_length == None:\n",
    "    max_token_length = MAX_TOKEN_LENGTH\n",
    "\n",
    "  doc = nlp(text)\n",
    "  words = [token.lower_ for token in doc]\n",
    "  embeddings = word2vec.get_vec(words).tolist()[:MAX_TOKEN_LENGTH]\n",
    "  padding = [[0] * 300] * (MAX_TOKEN_LENGTH - len(embeddings))\n",
    "\n",
    "  return embeddings + padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import protobufs.word_embedding_pb2 as EssayEmbedding\n",
    "\n",
    "import loader.ProtobufDataloader as ProtobufDataloader\n",
    "\n",
    "reload(ProtobufDataloader)\n",
    "\n",
    "def WordEmbeddingParser(protobuf):\n",
    "    embeddings = []\n",
    "\n",
    "    for embedding in protobuf.embedding:\n",
    "        embeddings.append([value for value in embedding.value])\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "word_embedding_protobuf_dataloader = ProtobufDataloader.ProtobufDataloader(DATA_DIR, protobuf=EssayEmbedding.EssayEmbedding, parser=WordEmbeddingParser)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_train_protobuf, real_test_protobuf, real_valid_protobuf = word_embedding_protobuf_dataloader.get(\"essayforum-384\", folders=[\"processed-v1\", \"cnn\", \"word-embedding\"], take=18000)\n",
    "fake_train_protobuf, fake_test_protobuf, fake_valid_protobuf = word_embedding_protobuf_dataloader.get(\"own-384\", folders=[\"processed-v1\", \"cnn\", \"word-embedding\"], take=18000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import loader.ProtobufGenerator as ProtobufGenerator\n",
    "reload(ProtobufDataloader)\n",
    "\n",
    "train_generator_protobuf = ProtobufGenerator.ProtobufGenerator([real_train_protobuf, fake_train_protobuf], [[0] * len(real_train_protobuf), [1] * len(fake_train_protobuf)], batch_size=32)\n",
    "test_generator_protobuf = ProtobufGenerator.ProtobufGenerator([real_test_protobuf, fake_test_protobuf], [[0] * len(real_test_protobuf), [1] * len(fake_test_protobuf)], batch_size=32)\n",
    "valid_generator_protobuf = ProtobufGenerator.ProtobufGenerator([real_valid_protobuf, fake_valid_protobuf], [[0] * len(real_valid_protobuf), [1] * len(fake_valid_protobuf)], batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = test_generator_protobuf[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inputs = Input(shape=shape[1:])\n",
    "rows_counts = [3, 5, 7, 9, 11, 13, 15]\n",
    "convs = []\n",
    "\n",
    "for rows_count in rows_counts:\n",
    "  conv2d = Convolution2D(filters=32, kernel_size=(rows_count, shape[2]), padding=\"valid\", data_format='channels_last')(inputs)\n",
    "  activation = Activation('relu')(conv2d)\n",
    "  maxpooling = MaxPooling2D(pool_size=(conv2d.shape[1], 1),  strides=1, padding='valid', data_format='channels_last')(activation)\n",
    "  dropout = Dropout(0.25)(maxpooling)\n",
    "\n",
    "  convs.append(dropout)\n",
    "\n",
    "convs = Concatenate(axis=1)(convs)\n",
    "\n",
    "flatten = Flatten()(convs)\n",
    "\n",
    "x = Dense(128, activation=\"relu\")(flatten)\n",
    "x = Dropout(0.25)(x)\n",
    "x = Dense(16, activation=\"relu\")(x)\n",
    "x = Dropout(0.1)(x)\n",
    "outputs = Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x=train_generator_protobuf, validation_data=valid_generator_protobuf, epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(x=test_generator_protobuf, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.predict(test_generator_protobuf)\n",
    "\n",
    "y_preds = list(map(lambda x: 1 if x[0] >= 0.5 else 0, result.tolist()))\n",
    "y_test = test_generator_protobuf.labels\n",
    "print(\"Model:\", \"CNN\")\n",
    "print(\"Confusion Matrix:\\n\", metrics.confusion_matrix(y_test, y_preds))\n",
    "print(\"Classification Report:\\n\", metrics.classification_report(y_test, y_preds))\n",
    "print(\"Accuracy: %.2f%%\" % (metrics.accuracy_score(y_test, y_preds) * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.dir import make_dir\n",
    "\n",
    "MODEL_PATH = make_dir(f'./models/cnn/model-cnn-{int(time.time())}.keras')\n",
    "model.save(MODEL_PATH)\n",
    "print(f\"[CNN] Saved at {MODEL_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
